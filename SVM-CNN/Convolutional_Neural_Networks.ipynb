{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> A Convolutional Neural Network Implementation </center> \n",
    "\n",
    "This tutorial focuses on building a CNN using Keras\n",
    " \n",
    "You will need to the following libraries to run this code:\n",
    "* __[numpy](http://www.numpy.org/)__\n",
    "* __[matplotlib](https://matplotlib.org/)__\n",
    "* __[tensorflow](https://www.tensorflow.org/)__\n",
    "* __[keras](https://keras.io/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images shape: (60000, 28, 28)\n",
      "train labels shape: (60000, 10)\n",
      "test images shape: (10000, 28, 28)\n",
      "test labels shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)\n",
    "print('train images shape:', train_images.shape)\n",
    "print('train labels shape:', train_labels.shape)\n",
    "print('test images shape:', test_images.shape)\n",
    "print('test labels shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data normalization \n",
    "Let's also normalize the data by subtracting the mean and dividing by the standard deviation. Notice that\n",
    "the mean and std of the training set are used in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype(np.float32)\n",
    "mean = np.mean(train_images)\n",
    "std = np.std(train_images)\n",
    "train_images = (train_images - mean) / std\n",
    "test_images = (test_images - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the CNN model\n",
    "Next, we will build a CNN model. Keras has a very simple API, in which we first instantiate a model, and then\n",
    "add layers to it. The first layer added should define the size of input the CNN should expect. We will add first\n",
    "a convolutional layer. We need to set the number of filters (kernels or units) in the layer, the size of the filters,\n",
    "the type of padding to use to handle the edges, and the activation function to use. There are many other\n",
    "things you can set in the layer (check keras docs). Notice how we define the input shape; these are 28x28\n",
    "pixel images, and the last entry is the number of channels. Grayscale images have 1 channel, RGB images\n",
    "have 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'\n",
    ", input_shape=(28, 28, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will add a max pooling layer, leaving the default parameters. This should reduce the size of the\n",
    "images to 14x14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPool2D())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another convolutional/maxpooling layers, reducing the size of the images to 7x7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPool2D())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to add some fully connected layers. To do this, first we need to 'flatten' the output of the\n",
    "previous layer, which has shape batch_sizex7x7x64, into batch_sizex3136."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a fully connected layer with 512 units, and ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(512, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we add another fully connected layer, let's also add a 'dropout' layer. Dropout is a recent\n",
    "regularization technique commonly used in deep networks. Regularization refers to methods that try to\n",
    "reduce a phenonmenon called 'overfitting', in which the model performs well on the training data, but poorly\n",
    "on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(512, activation='relu'))\n",
    "cnn.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will add the last layer. Since this is a classifcation problem with 10 classes, we will add a layer\n",
    "with 10 units and softmax activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our CNN model is complete, we need to compile it, specifying the optimizer to use and the loss\n",
    "function. We will use the Adam optimizer with default parameters, and, since this is a mutliclass\n",
    "classification problem, we will use the catrgorical cross entropy loss. We will use accuracy as a metric (this is just\n",
    "used to keep track of performance, but does not affect the training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready now to train the model. We can specify the batch size and the number of epochs for training.\n",
    "Notice that we need to modify the shape of the input first to add the channels dimension. Note that\n",
    "depending on what machine you are using, this might take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 68s 6ms/step - loss: 0.1638 - accuracy: 0.9491 - val_loss: 0.0561 - val_accuracy: 0.9816\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0653 - accuracy: 0.9815 - val_loss: 0.0333 - val_accuracy: 0.9906\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0492 - accuracy: 0.9857 - val_loss: 0.0418 - val_accuracy: 0.9879\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0409 - accuracy: 0.9884 - val_loss: 0.0303 - val_accuracy: 0.9915\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0365 - accuracy: 0.9897 - val_loss: 0.0289 - val_accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15dc1f57b88>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(train_images.reshape(-1, 28, 28, 1), \n",
    "        train_labels, \n",
    "        batch_size=32, \n",
    "        epochs=5,\n",
    "        validation_data=(test_images.reshape(-1, 28, 28, 1), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0289 - accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.028918975964188576, 0.9912999868392944]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(test_images.reshape(-1, 28, 28, 1),test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "It may be interesting to visualize the 64 3x3 CNN filters learned in the first convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1, 64)\n"
     ]
    }
   ],
   "source": [
    "weights = cnn.get_weights()[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcsAAAHBCAYAAAAGmZAhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWcElEQVR4nO3df6zWZR038OtOjQKxgfFj0zJJIG3iVpLYsiaCkDlLYColYCUCZaJpGizmlHZMsBQMU9EoqZBUTGcFFsgGDRbZFC0oQEUrQg0SzBbCzvPfvXMLnM/m9fGp59nr9dd1ztne9/sc73Pe+9La1Whvb28vAMBBve2/XQAA/tcZSwAIGEsACBhLAAgYSwAIGEsACBza2RfnzJmT8iKjR4+uzpg/f35Ck1Kuu+665vnwww9PyXz11VerM0aNGpXQpJQlS5Y0z8uXL0/JHDZsWHXGmWeemdCklGXLlrV8fPzxx6fkduvWrTpj+vTpCU1a3wtZ79Hrr7++OiPjfVBKKYMGDWqeG41GSmb37t1TcjLs2rWref7zn/+ckjlw4MDqjLfi/yXYu3fvlJwLLrigOmPcuHEJTUoZPHjwAT/vyRIAAsYSAALGEgACxhIAAsYSAALGEgACxhIAAsYSAALGEgACxhIAAsYSAALGEgACxhIAAsYSAALGEgACxhIAAp1e/nzjjTemvMjll19endGlS5f6IqX18ueJEyemZD7wwAPVGQ8++GBCk1Y9e/ZMycm4NHb27NkJTfa3YcOGlJyO74s3K+OS81Jaf97r169PyezXr191xllnnZXQpJRf/OIXzfOHPvShlMzHH3+8OqNv374JTVqtXbs2JWfv3r3VGTNnzkxoUsqMGTOa582bN6dkzps3rzoj66L0g/FkCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQCBTi9/Puyww1JepK2trTpj2rRpCU1affSjH03Jufrqq6szFi5cmNCk1c9//vOUnN/85jfVGf37909osr8Pf/jDKTn79u2rznjqqacSmrTK+N0pJef9tWfPnoQmra655pqUnIzLu7t3757QpFXv3r1TclatWlWd0aNHj4QmrY444oiUnIzfvxNOOCGhycEvu/dkCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAgUb7wa6FBgBKKZ4sASBkLAEgcGhnX+zTp0/Ki7z44ovVGQsWLEhoUspFF13UPHfr1i0l87XXXqvOuOWWW+qLlFKmTp3aPD///PMpmcccc0x1xlv1r/0nnXRSSs6vf/3r6oybbropoUkpN954Y/N87LHHpmRu3769OmP06NEJTUpZuHBh8zx37tyUzKOOOqo6I+P3uJRSxo0b1zx/6UtfSsm87bbbqjMajUZCk9bf5ay/yxmuvPLKlJwdO3Yc8POeLAEgYCwBIGAsASBgLAEgYCwBIGAsASBgLAEgYCwBIGAsASBgLAEgYCwBIGAsASBgLAEgYCwBIGAsASBgLAEg0Onlz3PmzEl5kS5dulRnnHvuuQlNWi1atCgl55xzzqnOePLJJxOatDriiCNSckaMGFGdMX369IQmpbS1tbV8/OMf/zglt1evXtUZs2bNSmjSevlzxsXppeRcbHzHHXckNGmVcel2KaWcd9551RkdL23Osnnz5pScbdu2VWdMmzYtoUmrPn36pORMnDixOqN79+4JTQ7OkyUABIwlAASMJQAEjCUABIwlAASMJQAEjCUABIwlAASMJQAEjCUABIwlAASMJQAEjCUABIwlAASMJQAEjCUABDq9/HnMmDEpL/KVr3ylOmP37t0JTUoZP35887x27dqUzMmTJ1dnHHPMMQlNSlmzZk3zvGfPnpTMiy66qDpj/fr19UUO4IUXXkjJufrqq6szjjvuuIQmrTIubS6llAceeKA64/vf/35Ck1ImTZrUPDcajZTMjIubb7/99oQmpaxevbp5Pv/881Myt2zZUp2xb9++hCatPvWpT6XkfP3rX6/OuOGGGxKaHJwnSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACDTa29vb/9slAOB/mSdLAAgc2tkX58+fn/Iil1xySXVGjx49EpqUsmPHjuZ56NChKZmPPfZYdcZb8YC/evXq9Mw366mnnkrJmTJlSsvHjUYjJTfj5z9r1qyEJqVcffXVzXPW93fllVdWZzzzzDMJTUpZsmRJ83z66aenZK5cubI644Mf/GB9kVLK008/3TwPHjw4JfPiiy+uzti9e3dCk1Kuuuqq5nnLli0pmdOmTavOuPXWWxOalNKnT58Dft6TJQAEjCUABIwlAASMJQAEjCUABIwlAASMJQAEjCUABIwlAASMJQAEjCUABIwlAASMJQAEjCUABIwlAASMJQAEOr38efjw4SkvknGxbscLY7Ns3rw5JWf79u3VGcuXL09oUsoZZ5zRPD/33HMpmStWrKjO6NevX0KT/d14440pOa+++mp1xpgxYxKatMq4OL2UUt75zndWZ/zhD39IaNJqyJAhKTmzZ8+uznjooYcSmrSaO3duSs5xxx1XndG7d++EJq2XP2ddeD558uTqjL59+yY0OfheebIEgICxBICAsQSAgLEEgICxBICAsQSAgLEEgICxBICAsQSAgLEEgICxBICAsQSAgLEEgICxBICAsQSAgLEEgECjvZObmQ855JCUF/nhD39YnfHHP/4xoUkpbW1tzfPNN9+cktmrV6/qjHnz5iU0KWXNmjUHPNc49dRTqzMajUZCk/0vZu3atWtK7l/+8pfqjBdeeCGhSSknnXRSSk5HRx99dHXG+PHjE5q0/g5myfjv9+yzzyY0KeW0005Lyelow4YN1Rljx45NaFLKE0880TxPmDAhJfP111+vzvjJT36S0OTgPFkCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkCg0f7Gq+cBgBaeLAEgYCwBIHBoZ1/83e9+l/Ii/fr1q854+eWXE5qUMmDAgJScjnr06FGdsWjRooQmpYwcObJ5bjQaKZnnn39+dcbw4cMTmpTyxS9+seXjp59+OiV3zpw51Rl33XVXQpNSOv4vI7t3707JvPvuu6szrrvuuoQmpezcubN5PuWUU1Iy29raqjP27NmT0KSUT37yk83zbbfdlpK5bt266oyhQ4cmNCll3LhxzfP06dNTMm+44YbqjH379iU0KeVtbzvwM6QnSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIdHr5c8dLPmts2LChOmPz5s0JTVpl9CqllF27dlVnDBs2LKFJq6zv7wMf+EB1xm9/+9uEJvt7+OGHU3LmzZtXnbF3796EJq2yLpT+6le/Wp2RccHyGy1evDgl55hjjqnOyLocuePlz/Pnz0/JXLVqVXXGD37wg/oib3DyySen5Lz//e+vzjjYpc1ZPFkCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQKDTy5+PPfbYlBdpNBrVGYccckhCk9YLek844YSUzEsvvbQ6o3///glNSnn22Web56yLkX/1q19VZ8yZMyehyf6XgP/nP/9Jyb399turMxYsWJDQpNU111yTknP//fdXZ4wePTqhSausvzHt7e3VGX379k1o0uqJJ55IyfnsZz9bnZH196Cj1atXp+T06NGjOuPiiy9OaHLwC9c9WQJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQKDRnnHFOAD8f8yTJQAEjCUABA7t7ItDhgxJeZFly5ZVZ6xduzahSSkjRoxonhuNRkrmqlWrqjOOOOKIhCalDBo0qHmeNWtWSuaaNWuqM1asWJHQpJRXXnml5eMjjzwyJfeOO+6ozvjb3/6W0KSUyy67LCWno9NOOy09883q+PuS8XMvpZRrr722OmPixIkJTUqZOXNm8zx27NiUzG9961vVGVm/K4cffnjzPG3atJTMlStXVmfcc8899UVKKf379z/g5z1ZAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkDAWAJAwFgCQMBYAkCg08ufMy79LaWUTZs2VWecfPLJCU1a3XrrrSk5H/vYx6ozRo0aldCklCVLljTPO3fuTMns3r17dcYbL23OcuGFF6bkjBkzpjpjzpw5CU1abdmyJSVn9erV1RlPPfVUQpNW9957b0rO3//+9+qMX/7ylwlNWmV9f0uXLq3OWLt2bUKTUgYOHNg8d+vWLSUz4+/fSy+9lNDE5c8A8KYZSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAg02tvb2w/6xUYj5UUyLta97777Epq0Gjt2bErOz372s+qMKVOm1BcppXznO99pnrP++61cubI6o3fv3vVFSinHH398y8eLFy9Oyd23b191xuc+97mEJqV0/JXM+v4uu+yy6oyPf/zjCU1af5ez3qMZlz/v2rUrocnBLw+uMW7cuOqMjRs3JjQpZd26dc3zwoULUzLHjx9fndHJlKXwZAkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAIFG+1t9vTQA/D/OkyUABA7t7IuPPvpoyosMHz68OmPAgAEJTUrZtGlT87x8+fKUzC9/+cvVGX/6058SmpTS8R8K5s6dm5LZpUuX6oyzzjoroUkp73nPe1o+3r59e0punz59qjN27NiR0KSUnj17puR01Gg0qjNmzJiR0KSU66+/vnlevXp1SmbG36qZM2cmNGn9Hcz4uZdSyvPPP1+d8de//jWhSSlDhgxpnrO+v4x/4Pz0pz+d0KSUhx566ICf92QJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAAFjCQABYwkAAWMJAIFOL38+88wzU17ka1/7WnXGP/7xj4QmrUaOHJmSM3r06OqMRYsWJTRpNXDgwJScESNGVGdccsklCU1KufPOO1s+7niZd43Zs2dXZxx11FEJTUq54oormucxY8akZK5bt646I+v91FHWpecZf6tOOOGEhCatMi41LqWUBx98sDoj629ox8ufn3vuuZTMjIvFMy4A74wnSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAINNo7uZ30fe97X8qLbN26tTpj2bJlCU1aL4nt2bNnSuakSZOqM2644YaEJq0ajUZKzpQpU6ozJk+enNCklEGDBrV8fOSRR6bk7tixozrjlFNOSWhSytq1a5vnbdu2pWRm5Nx///0JTUppa2trnjdu3JiSee2111ZnLF68OKFJq0svvTQlJ+Pn1KNHj4Qmpdx3333Nc9b3d+KJJ1Zn7N69O6FJKVddddUBP+/JEgACxhIAAsYSAALGEgACxhIAAsYSAALGEgACxhIAAsYSAALGEgACxhIAAsYSAALGEgACxhIAAsYSAALGEgACxhIAAo329vb2/3YJAPhf5skSAALGEgACh3b2xVGjRqW8yGuvvVad8eSTTyY0KWXbtm3N84QJE1Iy77nnnuqMt+Jfw7/xjW+k5Hzzm9+szmg0GglN9v85nXrqqSm5bW1t1RndunVLaFLKRz7ykZScjp599tnqjGeeeSahSSlnnHFG83zLLbekZL788svVGYsXL05oUsqmTZua5507d6ZkHnbYYdUZd999d0KTUqZOndo8f+ITn0jJ/Pa3v12dsXTp0oQmB/+76ckSAALGEgACxhIAAsYSAALGEgACxhIAAsYSAALGEgACxhIAAsYSAALGEgACxhIAAsYSAALGEgACxhIAAsYSAAKdXv787ne/O+VF7rzzzuqM+fPnJzRplXXh8oUXXlidce+99yY0KeWCCy5onocMGZKSuXXr1uqMa6+9NqHJ/tasWZOS8453vKM64+ijj05oUsrmzZub5+nTp6dkdu3atTrjn//8Z32R0nr5c9b3d/bZZ1dndPy5Z8l6f44bN646Y+XKlfVF3mDChAkpOVu2bKnOmDFjRkITlz8DwJtmLAEgYCwBIGAsASBgLAEgYCwBIGAsASBgLAEgYCwBIGAsASBgLAEgYCwBIGAsASBgLAEgYCwBIGAsASDQaO/kBuRGo5HyIqeffnpKToYVK1Y0z+9973tTMu+6667qjAcffDChSSnf+973muesC5evvPLK6oxt27YlNCll4MCBLR9nvUd79epVnbF8+fKEJqWceOKJzfOwYcNSMidNmlSdcd555yU0ab10PetC4m7dulVnDB48OKFJq6z357///e/qjIwLzt/opptuSslZt25ddcbixYsTmhycJ0sACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAgYSwAIGEsACBhLAAg02jteWw4A7MeTJQAEjCUABA79v/EijUajOuPcc89NaFLKkiVLmufVq1enZK5bt64646WXXkpoUkpbW1vz/Mgjj6Rknn322dUZ3/3udxOalHLppZe2fLx06dKU3JEjR1Zn3H///QlNShkzZkzz/Morr6RkHn300dUZc+fOTWhSyuc///mUnI4yuk2dOjWhSSlvxf+yNXTo0OqMyy+/vL5IKeWcc85pnjP+tpeS8zPbtGlTQpNS+vfvf8DPe7IEgICxBICAsQSAgLEEgICxBICAsQSAgLEEgICxBICAsQSAgLEEgICxBICAsQSAgLEEgICxBICAsQSAgLEEgECnlz9v3bo15UXGjh1bnTF9+vSEJq3Wr1+fkvPCCy9UZ9x8880JTVovf168eHFKZq9evaozXn/99YQm+3v729+ekpPxXli5cmV9kdJ6+fO73vWulMxXX321OiPrcuSOlz/v27cvJfP3v/99dUbGBdlv9IUvfCEl58UXX6zO+OlPf5rQpPXy56z/fj179qzOGDBgQEKTUtauXXvAz3uyBICAsQSAgLEEgICxBICAsQSAgLEEgICxBICAsQSAgLEEgICxBICAsQSAgLEEgICxBICAsQSAgLEEgICxBIBAp5c/Z1w4WkopmzZtqs74zGc+U1+klLJ58+bm+bHHHkvJ/Ne//lWd0d7entCk1Y9+9KOUnGHDhlVnXHHFFQlN9nfGGWek5MyaNas6Y8aMGQlNWu3duzclZ+PGjdUZXbt2TWjS6tFHH03Jefjhh6szHnnkkYQmrRYsWJCS8/jjj1dnZF2U3lHv3r1TcpYsWVKdkfVeOhhPlgAQMJYAEDCWABAwlgAQMJYAEDCWABAwlgAQMJYAEDCWABAwlgAQMJYAEDCWABAwlgAQMJYAEDCWABAwlgAQMJYAEGi0t7e3/7dLAMD/Mk+WABAwlgAQMJYAEDCWABAwlgAQMJYAEPg/o2ZUXjT+YEYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x576 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(facecolor='w', edgecolor='w', figsize=(8, 8))\n",
    "fig.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "for i in range(64):\n",
    " sp = fig.add_subplot(8, 8, i+1)\n",
    " sp.set_axis_off()\n",
    " plt.imshow(weights[:, :, 0, i])\n",
    " plt.set_cmap('gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not very clear, the filters learn to look for elemntary concepts like oriented edges, symmetries, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
