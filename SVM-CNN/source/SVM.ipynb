{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  type  \n",
      "0      9.4        5     0  \n",
      "1      9.8        5     0  \n",
      "2      9.8        5     0  \n",
      "3      9.8        6     0  \n",
      "4      9.4        5     0  \n",
      "(6497, 13)\n"
     ]
    }
   ],
   "source": [
    "# data from white wine and red wine is already merged into a single winequality datasheet\n",
    "df = pd.read_csv(\"winequality.csv\")\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (5197, 10)\n",
      "Training labels shape: (5197,)\n",
      "Test set shape: (1300, 10)\n",
      "Test label shape: (1300,)\n"
     ]
    }
   ],
   "source": [
    "# column 1 -> 11 = features, column 12 = quality, column 13 = wine category\n",
    "(x, y) = df.iloc[:, 0:10], df.iloc[:, 11]\n",
    "\n",
    "# normalization\n",
    "n_x = MinMaxScaler().fit_transform(x)\n",
    "\n",
    "# 80% for training, 20% for testing \n",
    "x_train, x_test, y_train, y_test = train_test_split(n_x, y, test_size=0.20)\n",
    "\n",
    "print('Training set shape: {}'.format(x_train.shape))\n",
    "print('Training labels shape: {}'.format(y_train.shape))\n",
    "print('Test set shape: {}'.format(x_test.shape))\n",
    "print('Test label shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(C, kernal, g=\"scale\"):\n",
    "    svm = SVC(C=C, kernel=kernal, gamma=g)\n",
    "    svm.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = svm.predict(x_test)\n",
    "    acc_svm = metrics.accuracy_score(y_test, y_pred)\n",
    "    print('SVM Accuracy for {} at C = {} and gamma = {}:'.format(kernal, C, g), acc_svm)\n",
    "    \n",
    "    # cm_svm = confusion_matrix(y_test, y_pred)\n",
    "    # class_names = ['0', '1', '2', '3', '4', '5', '6']\n",
    "\n",
    "    # print(cm_svm)\n",
    "    # print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "    # disp1 = ConfusionMatrixDisplay(confusion_matrix=cm_svm)\n",
    "    # disp1.plot()\n",
    "    return acc_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy for rbf at C = 1 and gamma = scale: 0.546923076923077\n",
      "SVM Accuracy for rbf at C = 10 and gamma = scale: 0.5930769230769231\n",
      "SVM Accuracy for rbf at C = 50 and gamma = scale: 0.5838461538461538\n",
      "SVM Accuracy for rbf at C = 100 and gamma = scale: 0.5892307692307692\n",
      "SVM Accuracy for poly at C = 1 and gamma = scale: 0.5592307692307692\n",
      "SVM Accuracy for poly at C = 10 and gamma = scale: 0.5738461538461539\n",
      "SVM Accuracy for poly at C = 50 and gamma = scale: 0.573076923076923\n",
      "SVM Accuracy for poly at C = 100 and gamma = scale: 0.5746153846153846\n",
      "SVM Accuracy for linear at C = 1 and gamma = scale: 0.5069230769230769\n",
      "SVM Accuracy for linear at C = 10 and gamma = scale: 0.5269230769230769\n",
      "SVM Accuracy for linear at C = 20 and gamma = scale: 0.5238461538461539\n",
      "SVM Accuracy for linear at C = 30 and gamma = scale: 0.5261538461538462\n"
     ]
    }
   ],
   "source": [
    "C1 = [1, 10, 50, 100]\n",
    "C2 = [1, 10, 20, 30]\n",
    "\n",
    "result = np.empty((3, 4), dtype=float)\n",
    "\n",
    "# RBF Kernal\n",
    "for idc, C in enumerate(C1): \n",
    "    acc_svm = SVM(C, 'rbf')\n",
    "    result[0, idc] = acc_svm\n",
    "    \n",
    "# Poly Kernal \n",
    "for idc, C in enumerate(C1): \n",
    "    acc_svm = SVM(C, 'poly')\n",
    "    result[1, idc] = acc_svm\n",
    "    \n",
    "# Linear Kernal\n",
    "for idc, C in enumerate(C2): \n",
    "    acc_svm = SVM(C, 'linear')\n",
    "    result[2, idc] = acc_svm\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy result in table, the column represents the different regularization parameters used and the rows represents the different kernal used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            C[0]      C[1]      C[2]      C[3]\n",
      "RBF     0.546923  0.593077  0.583846  0.589231\n",
      "Poly    0.559231  0.573846  0.573077  0.574615\n",
      "Linear  0.506923  0.526923  0.523846  0.526154\n"
     ]
    }
   ],
   "source": [
    "# create the dataframe\n",
    "df = pd.DataFrame(result, columns=[\"C[0]\", \"C[1]\", \"C[2]\", \"C[3]\"],\n",
    "                index=[\"RBF\", \"Poly\", \"Linear\"])\n",
    "\n",
    "# print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b\n",
    "\n",
    "Comparing the accuracy result from the table above, we can see that RBF kernal performed the best overall with an average accuracy of 57.826925% compared to 57.019225% for poly kernal and only 52.12115% for linear kernal. This make sense since guassian kernal is generally the perfered function in svm. It is suitable for non-linear data and helps to make proper separation when there is no prior knowledge of data. On the otherhand the linear kernal is the most basic kernal and is mostly preferred for text-classification and linear kernal is just a more generalized representation of the linear kernal. Furthermore, we can see that increasing the regularization parameter c sees improvement in training and test accuracy as well, since a higher value of the regularization parameter will penalize the model more for misclassifying training examples and lead to a smaller margin, while a lower value of the regularization parameter will allow more margin violations and lead to a larger margin."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c Improving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy for rbf at C = 1 and gamma = 0.1: 0.49538461538461537\n",
      "SVM Accuracy for rbf at C = 1 and gamma = 1: 0.5169230769230769\n",
      "SVM Accuracy for rbf at C = 1 and gamma = 10: 0.5776923076923077\n",
      "SVM Accuracy for rbf at C = 1 and gamma = 100: 0.6215384615384615\n",
      "SVM Accuracy for rbf at C = 10 and gamma = 0.1: 0.5246153846153846\n",
      "SVM Accuracy for rbf at C = 10 and gamma = 1: 0.5284615384615384\n",
      "SVM Accuracy for rbf at C = 10 and gamma = 10: 0.5884615384615385\n",
      "SVM Accuracy for rbf at C = 10 and gamma = 100: 0.6376923076923077\n",
      "SVM Accuracy for rbf at C = 50 and gamma = 0.1: 0.5223076923076924\n",
      "SVM Accuracy for rbf at C = 50 and gamma = 1: 0.5453846153846154\n",
      "SVM Accuracy for rbf at C = 50 and gamma = 10: 0.5953846153846154\n",
      "SVM Accuracy for rbf at C = 50 and gamma = 100: 0.64\n",
      "SVM Accuracy for rbf at C = 100 and gamma = 0.1: 0.5269230769230769\n",
      "SVM Accuracy for rbf at C = 100 and gamma = 1: 0.5592307692307692\n",
      "SVM Accuracy for rbf at C = 100 and gamma = 10: 0.6046153846153847\n",
      "SVM Accuracy for rbf at C = 100 and gamma = 100: 0.64\n"
     ]
    }
   ],
   "source": [
    "C = [1, 10, 50, 100]\n",
    "gamma = [10**-1, 10**0, 10*1, 10**2]\n",
    "\n",
    "result = np.empty((len(C), len(gamma)), dtype=float)\n",
    "\n",
    "for idc, c in enumerate(C): \n",
    "    for idg, g in enumerate(gamma): \n",
    "        acc_svm = SVM(c, 'rbf', g)\n",
    "        result[idc, idg] = acc_svm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy result for rbf kernal in table, the column represents the different regularization parameters used and the rows represents the different gamma parameters used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0.1       1.0       10.0      100.0\n",
      "1    0.495385  0.516923  0.577692  0.621538\n",
      "10   0.524615  0.528462  0.588462  0.637692\n",
      "50   0.522308  0.545385  0.595385  0.640000\n",
      "100  0.526923  0.559231  0.604615  0.640000\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(result, columns=gamma,\n",
    "                index=C)\n",
    "\n",
    "# print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that not only does a larger regularization parameter kernal improves the model, but also does a larger gamma parameter. In general a high gamma value may lead to overfitting, where the model captures noise in the training data and performs poorly on new, unseen data. A low gamma value may lead to underfitting, where the model is too simple to capture the underlying patterns in the data and performs poorly on both the training and testing data. Here since the number of data in the wine-dataset is very limited and consist of a low number of feature, a higher gamma is more suitable in order to prevent underfitting. Here we reached the optimal accuracy of 0.64 using C = 100 and gamma = 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
