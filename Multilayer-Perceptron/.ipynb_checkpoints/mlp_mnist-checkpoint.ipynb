{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Multi-Layer Perceptron\n",
    "\n",
    "In this tutorial, we will work on creating a multi-layer perceptron to classify hand-written digits from the MNIST dataset using Tensorflow and Keras. (Keras is part of Tensorflow as of TF 2.0)\n",
    "\n",
    "We will explore the effect of the size of the training and testing sets as well as overfitting & underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python.keras.activations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2676\\1879248592.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# make it easier to understand by importing the required libraries within keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\keras\\activations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0melu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexponential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.keras.activations'"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "# make it easier to understand by importing the required libraries within keras\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and explore the dataset\n",
    "First step is to load the MNIST dataset. Keras has functions to directly download some famous __[datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)__. The first time you run this command, it will install the dataset. \n",
    "\n",
    "The dataset contains 70,000 samples of handwritten digits. Each sample is a single channel (i.e. grayscale) 28x28 (pixels) image of a hand-written digit.\n",
    "\n",
    "We have 10 classes, for the digits from 0 to 9. More information on the dataset can be found in __[Yann LeCun's MNIST Database](http://yann.lecun.com/exdb/mnist/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2676\\2583427149.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# explore the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training set shape: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training labels shape: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# explore the dataset\n",
    "print('Training set shape: {}'.format(x_train.shape))\n",
    "print('Training labels shape: {}'.format(y_train.shape))\n",
    "print('Test set shape: {}'.format(x_test.shape))\n",
    "print('Test labels shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the training set has 60k images while the test set has 10k.\n",
    "\n",
    "Let us have a closer look at the dataset before we start working with it. It is always a good practice to check the size, the range of values for the input and output, and visualize a few samples.\n",
    "\n",
    "It is also a good idea to check the format of the labels. They may be given as integers of on-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2676\\3049071856.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sample labels are: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The classes are: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The values in the input data range from {} to {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "labels = set(y_train)\n",
    "\n",
    "print('Sample labels are: {}'.format(y_train[:20]))\n",
    "print('The classes are: {}'.format(labels))\n",
    "print('The values in the input data range from {} to {}'.format(x_train.min(),x_train.max()))\n",
    "\n",
    "# Visualize a sample from each class\n",
    "i = 0\n",
    "for label in labels:\n",
    "    imgs = x_test[y_test==label]\n",
    "    img = imgs[0]\n",
    "    \n",
    "    # plot\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title('Class %.f' %label)\n",
    "    plt.axis('off')\n",
    "    plt.subplots_adjust(wspace = 0.4)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10 classes as expected (0-9). The values of the input range from 0-255, and this seems correct since the images are all in grayscale.\n",
    "\n",
    "If you are using a pre-trained model, it is important to know what normalization was used on the input in training and use the same on your input data. Likewise, your training and testing data should be normalized in the same way.\n",
    "\n",
    "We usually normalize images to have pixel values in the range of 0-1 or 0 mean and unit variance.\n",
    "\n",
    "In this example, we will normalize using min-max normalization which will set all our values to have a minimum of 0 and a maximum of one.\n",
    "\n",
    "$x_{i}' = \\frac{x_{i}-min(x)}{max(x)-min(x)}$\n",
    "\n",
    "Where x is the feature (column). We already know that the minimum value is 0 and the maximum is 255 for all features, so we'll directly apply that in our equation. \n",
    "\n",
    "An alternative would be to use __[MinMax scaler from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html#sklearn.preprocessing.minmax_scale)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2676\\834118307.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The values in the input data range from {} to {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype(float)/255\n",
    "x_test = x_test.astype(float)/255\n",
    "print('The values in the input data range from {} to {}'.format(x_train.min(),x_train.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using softmax as the activation function in the last layer, which outputs our labels one-hot encoded. Generally, we should convert our input labels to the same format. \n",
    "\n",
    "Although tf and keras do that automatically and this is not required in this example, we will go ahead with it to conform to the general case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "Old labels: [5 0 4]\n"
     ]
    }
   ],
   "source": [
    "# make sure that the labels are still integers (this cell has not been run before)\n",
    "if len(y_train.shape)==1:\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(y_train[0:3])\n",
    "print('Old labels: {}'.format(np.argmax(y_train[:3], axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Defining the MLP Model\n",
    "Now that we have loaded, explored, and preprocessed the dataset, we are ready to create the MLP that we will train to classify the dataset.\n",
    "\n",
    "We will use Keras Sequential API within tensorflow. We will build an MLP with 2 hidden layers (and an input and output layer). All layers in an MLP are fully connected, sometimes also called 'dense' layers.\n",
    "\n",
    "We will first 'Flatten' the input to be one column of (28x28) 784 values. Then we expect the first Dense layer to allow for an input of size 784. \n",
    "\n",
    "We also need to define the number of neurons in each layer. For now, we will go with 64 neurons in each layer. We will also use a Sigmoid activation function for both layers.\n",
    "\n",
    "For the output layer, we need to define the number of units and the type of activation function. Since we will be using a softmax activation function, then we need 10 units in the output layer.\n",
    "\n",
    "In the sequential API from Keras, we define the layers as stacked after one another. You can define them all in one line, or in separate calls to the function 'add'. Just remember that the layers are put in the order you define them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model type (still empty)\n",
    "mlp = keras.models.Sequential()\n",
    "\n",
    "# add a layer that just flattens the input (no weights here)\n",
    "mlp.add(Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# add the first hidden layer with 64 neurons, an activation function of sigmoid, and an input size of 784\n",
    "mlp.add(Dense(64, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# add the second hidden layer with 64 neurons and sigmoid activation function\n",
    "mlp.add(Dense(64, activation='sigmoid'))\n",
    "\n",
    "# add the output layer with 10 units and Softmax activation function\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "## You can also create the same exact network at once:\n",
    "# model = tf.keras.models.Sequential([\n",
    "#   Flatten(input_shape=(28, 28)),\n",
    "#   Dense(64, activation='sigmoid', input_shape=(784,)),\n",
    "#   Dense(64, activation='sigmoid'),\n",
    "#   Dense(10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "print(mlp.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the network all set. \n",
    "\n",
    "We need to define the optimization process:\n",
    "* What is the loss function that we will try to minimize\n",
    "* What optimizer do we want to use\n",
    "* Metrics we would like to monitor during training (these do not affect the optimization)\n",
    "\n",
    "The loss function depends on the problem we are trying to solve. For multi-class classification, we will use the cross-entropy loss for categorical data: *categorical_crossentropy* in keras.\n",
    "\n",
    "As for the optimizer, we will be using Adam  (a variant of gradient descent) with the default parameters.\n",
    "\n",
    "We will also monitor the accuracy of prediction on the training set using the accuracy metric.\n",
    "\n",
    "To add these parameters, we will compile the model and pass them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training the Model\n",
    "We can now begin our training.\n",
    "\n",
    "Suppose our training set is made of 200 samples only. Let's begin training with that for 100 epochs and see how our network behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = mlp.fit(x_train[:200], y_train[:200], epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# plot the training accuracy\n",
    "plt.plot(h.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very quickly, we reach an accuracy of 100%. You may be tempted to think that your model is good and it can perform well on future data. However, this accuracy is on the training data, and it may not be representative of test accuracy. This can happen for different reasons (mismatch between training and test distributions, limited data, overfitting, etc.)\n",
    "\n",
    "When training a model, what you need to be monitoring is the loss/accuracy on a held-out set, called the **validation set**. So, let’s evaluate our model on the test set to see how it does there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy: %.2f %%'%(100*mlp.evaluate(x_test,y_test, verbose=0)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model does poorly on the test set, and compare that to how well it does on the training set. The reason in this case is that we did not use enough training data, and the model did not generalize enough.\n",
    "\n",
    "Let's use the whole training set and take 25% of it as a validation set to monitor the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = mlp.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.25, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h.history['accuracy'])\n",
    "plt.plot(h.history['val_accuracy'], 'r')\n",
    "plt.legend(['train acc', 'val acc'])\n",
    "print('train acc: %.2f %% \\nval acc: %.2f %%'%(h.history['accuracy'][-1]*100,h.history['val_accuracy'][-1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the test set again to see if this changes anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy: %.2f %%'%(100*mlp.evaluate(x_test,y_test, verbose=0)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the validation accuracy is a better estimate of the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "In some cases, a model could be overfit the training data. You can idenitfy this situation if you notice your training accuracy increasing while the validation accuracy is decreasing. \n",
    "* Adding more training data is one way to counter this issue, but that is not always feasible.\n",
    "* Another approach is to simplify your model: if your model is a very high capacity (many layers/units) while your data is limited, you are probably going to overfit. \n",
    "\n",
    "To see this in action, let’s look at this toy regression example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the input is 2-dimensional ($x_{1}$, $x_{2}$) and that $y = 3x_{1} + 2x_{2}$ + 2n, where n is some random\n",
    "noise. \n",
    "\n",
    "Let’s generate 100 samples from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 6*np.random.rand(100,2) -3\n",
    "y = 3*x[:,0] + 2*x[:,1] + 2*np.random.rand(100,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create an MLP for this regression problem, this will follow the same steps bit with a few differences in the loss and kind of activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = keras.models.Sequential([\n",
    "    Dense(1024, activation='relu', input_shape=(2,)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "          ])\n",
    "    \n",
    "mlp.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model and plot the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = mlp.fit(x, y, epochs=2000, batch_size=50, validation_split=0.5, verbose=0)\n",
    "\n",
    "plt.plot(np.log10(h.history['loss']))\n",
    "plt.plot(np.log10(h.history['val_loss']), 'r')\n",
    "plt.legend(['train loss', 'val loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the validation goes down at first, and then starts to go back up. This is an indication of overfitting. One way to counter this (aside from adding more data), is to use early stopping: stop training when the validation loss begins to increase.\n",
    "\n",
    "Another approach, as we mentioned earlier, is to simplify our model, say by reducing the number of units from 1024 to 64 and removing the second hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp2 = keras.models.Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(2,)),\n",
    "    Dense(1, activation='linear')\n",
    "          ])\n",
    "\n",
    "mlp2.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "\n",
    "h = mlp2.fit(x, y, epochs=2000, batch_size=50, validation_split=0.5, verbose=0)\n",
    "plt.plot(np.log10(h.history['loss']))\n",
    "plt.plot(np.log10(h.history['val_loss']), 'r')\n",
    "plt.legend(['train loss', 'val loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "73978d9442b59977790309cae14a463d3d21fefd825620a28d11a9727adbb2fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
